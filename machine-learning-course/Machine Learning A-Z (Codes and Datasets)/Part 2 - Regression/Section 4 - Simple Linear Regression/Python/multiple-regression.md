Multiple linear regression is a statistical method used to analyze the relationship between two or more independent variables (predictors) and a dependent variable. It extends the simple linear regression model, which involves only one independent variable.

In multiple linear regression, the relationship between the independent variables (X1, X2, ..., Xn) and the dependent variable (Y) is expressed by the following equation:

Y = β0 + β1*X1 + β2*X2 + ... + βn*Xn + ε

Where:
- Y is the dependent variable.
- X1, X2, ..., Xn are the independent variables.
- β0 is the intercept (the value of Y when all independent variables are zero).
- β1, β2, ..., βn are the coefficients (slopes) representing the change in Y for a one-unit change in the corresponding independent variable, holding other variables constant.
- ε is the error term, representing the difference between the observed and predicted values of Y.

The goal of multiple linear regression is to estimate the coefficients (β) that best fit the observed data. This is typically done by minimizing the sum of squared differences between the observed and predicted values of Y, a method known as ordinary least squares (OLS) regression.

Multiple linear regression is widely used in various fields such as economics, finance, social sciences, and engineering for predicting outcomes and understanding the relationships between variables. It assumes linearity between the independent and dependent variables, as well as other assumptions like normality of residuals and homoscedasticity, which should be checked before interpreting the results.